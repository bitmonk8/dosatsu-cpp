#!/usr/bin/env python3
"""
Profile Analysis Script

This script processes .etl files generated by etwprof using xperf to extract
performance hotspots and generate analysis reports for Dosatsu optimization.
"""

import sys
import os
import argparse
import subprocess
import json
import tempfile
import csv
from pathlib import Path
from datetime import datetime


def get_xperf_path():
    """Get the path to xperf.exe from Windows Performance Toolkit."""
    default_paths = [
        "C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit\\xperf.exe",
        "C:\\Program Files\\Windows Kits\\10\\Windows Performance Toolkit\\xperf.exe",
        "C:\\Windows\\System32\\xperf.exe"
    ]
    
    for path in default_paths:
        if os.path.exists(path):
            return path
    
    # Try to find in PATH
    try:
        result = subprocess.run(["where", "xperf"], capture_output=True, text=True)
        if result.returncode == 0:
            return result.stdout.strip().split('\n')[0]
    except:
        pass
    
    return None


def validate_etl_file(etl_path):
    """Validate that the ETL file exists and is readable."""
    etl_path = Path(etl_path)
    
    if not etl_path.exists():
        print(f"[ERROR] ETL file not found: {etl_path}")
        return False
    
    if etl_path.stat().st_size == 0:
        print(f"[ERROR] ETL file is empty: {etl_path}")
        return False
    
    return True


def extract_cpu_sampling_data(etl_path, xperf_path, output_dir):
    """Extract CPU sampling data from ETL file using xperf."""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate output files
    summary_file = output_dir / f"{Path(etl_path).stem}_summary.txt"
    csv_file = output_dir / f"{Path(etl_path).stem}_stacks.csv"
    butterfly_file = output_dir / f"{Path(etl_path).stem}_butterfly.html"
    
    symbol_path = "artifacts\\debug\\bin\\;artifacts\\release\\bin\\;srv*C:\\symbols*http://msdl.microsoft.com/download/symbols"

    env = os.environ.copy()
    env['_NT_SYMBOL_PATH'] = symbol_path

    print(f"Analyzing {Path(etl_path).name}...")
    
    # Extract CPU sampling summary
    try:
        print("   Extracting CPU sampling summary...")
        summary_cmd = [
            xperf_path, "-i", str(etl_path),
            "-symbols",
            "-o", str(summary_file),
            "-a", "profile"
        ]
        
        result = subprocess.run(summary_cmd, capture_output=True, text=True, env=env)
        if result.returncode != 0:
            print(f"   [WARNING] Summary extraction failed: {result.stderr}")
        else:
            print(f"   Summary saved: {summary_file.name}")
    
    except Exception as e:
        print(f"   [ERROR] Failed to extract summary: {e}")
    
    # Extract detailed stack traces to CSV (legacy method)
    try:
        print("   Extracting stack trace data...")
        stacks_cmd = [
            xperf_path, "-i", str(etl_path),
            "-symbols",
            "-o", str(csv_file),
            "-a", "profile", "-detail"
        ]
        
        result = subprocess.run(stacks_cmd, capture_output=True, text=True, env=env)
        if result.returncode != 0:
            print(f"   [WARNING] Stack extraction failed: {result.stderr}")
        else:
            print(f"   Stack data saved: {csv_file.name}")
    
    except Exception as e:
        print(f"   [ERROR] Failed to extract stacks: {e}")
    
    # Extract detailed call stack information using butterfly view
    try:
        print("   Extracting detailed call stack information...")
        butterfly_cmd = [
            xperf_path, "-i", str(etl_path),
            "-symbols",
            "-o", str(butterfly_file),
            "-a", "stack", "-butterfly"
        ] 
        
        result = subprocess.run(butterfly_cmd, capture_output=True, text=True, env=env)
        if result.returncode != 0:
            print(f"   [WARNING] Butterfly analysis failed: {result.stderr}")
        else:
            print(f"   Call stack analysis saved: {butterfly_file.name}")
            return butterfly_file
    
    except Exception as e:
        print(f"   [ERROR] Failed to extract butterfly analysis: {e}")
    
    # Fallback to CSV file if butterfly failed
    return csv_file if csv_file.exists() else None


def parse_butterfly_html(butterfly_file):
    """Parse butterfly HTML output to extract performance data."""
    import re
    
    try:
        with open(butterfly_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # Extract modules by exclusive hits data - look for the specific table section
        modules_section_match = re.search(r'<a id=\'TblME\'><h2>Modules by Exclusive Hits</h2></a>(.*?)<a id=\'TblMI\'>', html_content, re.DOTALL)
        modules_data = []
        modules_matches = []
        
        if modules_section_match:
            modules_section = modules_section_match.group(1)
            # Extract module data from this section
            modules_pattern = r'<tr><td><p><a id=\'[^\']+\'[^>]*>([^<]+)</a></p></td><td>(\d+)</td><td>([\d.]+)%</td>'
            modules_matches = re.findall(modules_pattern, modules_section)
        
        for module_name, hits, percentage in modules_matches:
            modules_data.append({
                'module': module_name,
                'hits': int(hits),
                'percentage': float(percentage)
            })
        
        # Extract function call stack data from the butterfly view
        # Look for the "Functions by Multi-Inclusive Hits with Callers and Callees" section
        functions_pattern = r'<a id=\'#([^\']+)\' href=\'#([^\']+)\'>([^<]+)</a>!</\*\*\*unknown\*\*\*</td><td>(\d+)</td><td>([\d.]+)%</td>'
        functions_matches = re.findall(functions_pattern, html_content)
        
        functions_data = []
        for func_id, module_id, module_name, hits, percentage in functions_matches:
            functions_data.append({
                'function_id': func_id,
                'module': module_name,
                'hits': int(hits),
                'percentage': float(percentage)
            })
        
        return {
            'modules': modules_data,
            'functions': functions_data,
            'total_samples': sum(mod['hits'] for mod in modules_data) if modules_data else 0
        }
        
    except Exception as e:
        print(f"   [ERROR] Failed to parse butterfly HTML: {e}")
        return None


def analyze_hotspots(analysis_file, output_dir, top_n=20):
    """Analyze the analysis file to identify performance hotspots."""
    if not analysis_file or not analysis_file.exists():
        return None
    
    output_dir = Path(output_dir)
    hotspots_file = output_dir / f"{analysis_file.stem}_hotspots.json"
    
    print("   Analyzing performance hotspots...")
    
    try:
        # Check if this is a butterfly HTML file or CSV file
        if analysis_file.suffix.lower() == '.html':
            # Parse butterfly HTML output
            butterfly_data = parse_butterfly_html(analysis_file)
            if not butterfly_data:
                return None
            
            # Extract data from butterfly analysis
            module_samples = {mod['module']: mod['hits'] for mod in butterfly_data['modules']}
            function_samples = {f"{func['module']}!***unknown***": func['hits'] for func in butterfly_data['functions']}
            total_samples = butterfly_data['total_samples']
            
        else:
            # Parse CSV data (legacy method)
            function_samples = {}
            module_samples = {}
            total_samples = 0
            
            # Read and parse CSV data
            # xperf CSV format: Process Name ( PID), Weight, Usage %, Module Name
            with open(analysis_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
                # Skip header and parse data lines
                for line in lines[1:]:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # Parse CSV line manually (comma-separated, might have spaces)
                    parts = [part.strip() for part in line.split(',')]
                    if len(parts) < 4:
                        continue
                    
                    process_info = parts[0]
                    weight_str = parts[1]
                    usage_str = parts[2]
                    module = parts[3]
                    
                    # Skip idle processes
                    if 'Idle' in process_info:
                        continue
                    
                    # Only process our target dosatsu process
                    if 'dosatsu_cpp.exe' not in process_info:
                        continue
                    
                    try:
                        weight = int(weight_str)
                        usage_pct = float(usage_str)
                    except (ValueError, TypeError):
                        continue
                    
                    total_samples += weight
                    
                    # Use module as the function name for now (we don't have detailed function info)
                    function = module
                    
                    # Aggregate by function/module
                    if function not in function_samples:
                        function_samples[function] = 0
                    function_samples[function] += weight
                    
                    # Aggregate by module
                    if module not in module_samples:
                        module_samples[module] = 0
                    module_samples[module] += weight
        
        # Sort by sample count
        top_functions = sorted(function_samples.items(), key=lambda x: x[1], reverse=True)[:top_n]
        top_modules = sorted(module_samples.items(), key=lambda x: x[1], reverse=True)[:top_n]
        
        # Create analysis results
        analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "etl_file": str(analysis_file.stem),
            "total_samples": total_samples,
            "top_functions": [
                {
                    "function": func,
                    "samples": samples,
                    "percentage": (samples / total_samples * 100) if total_samples > 0 else 0
                }
                for func, samples in top_functions
            ],
            "top_modules": [
                {
                    "module": module,
                    "samples": samples,
                    "percentage": (samples / total_samples * 100) if total_samples > 0 else 0
                }
                for module, samples in top_modules
            ]
        }
        
        # Save analysis
        with open(hotspots_file, 'w') as f:
            json.dump(analysis_results, f, indent=2)
        
        print(f"   Analysis complete: {hotspots_file.name}")
        return hotspots_file
        
    except Exception as e:
        print(f"   [ERROR] Failed to analyze hotspots: {e}")
        return None


def generate_report(analysis_files, output_dir):
    """Generate a summary report from multiple analysis files."""
    output_dir = Path(output_dir)
    report_file = output_dir / f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    
    print(f"Generating performance report...")
    
    try:
        with open(report_file, 'w') as f:
            f.write("# Dosatsu Performance Analysis Report\n\n")
            f.write(f"Generated: {datetime.now().isoformat()}\n\n")
            
            f.write("## Executive Summary\n\n")
            f.write("This report analyzes performance hotspots identified through ETW profiling of Dosatsu examples.\n\n")
            
            f.write("## Analysis Results\n\n")
            
            for analysis_file in analysis_files:
                if not analysis_file or not analysis_file.exists():
                    continue
                
                try:
                    with open(analysis_file, 'r') as af:
                        data = json.load(af)
                    
                    f.write(f"### {data['etl_file']}\n\n")
                    f.write(f"- **Total Samples**: {data['total_samples']:,}\n")
                    f.write(f"- **Analysis Time**: {data['timestamp']}\n\n")
                    
                    f.write("#### Top Functions by CPU Usage\n\n")
                    f.write("| Function | Samples | Percentage |\n")
                    f.write("|----------|---------|------------|\n")
                    for func_data in data['top_functions'][:10]:
                        f.write(f"| {func_data['function']} | {func_data['samples']:,} | {func_data['percentage']:.2f}% |\n")
                    f.write("\n")
                    
                    f.write("#### Top Modules by CPU Usage\n\n")
                    f.write("| Module | Samples | Percentage |\n")
                    f.write("|--------|---------|------------|\n")
                    for mod_data in data['top_modules'][:10]:
                        f.write(f"| {mod_data['module']} | {mod_data['samples']:,} | {mod_data['percentage']:.2f}% |\n")
                    f.write("\n")
                    
                except Exception as e:
                    f.write(f"Error processing {analysis_file}: {e}\n\n")
            
            f.write("## Recommendations\n\n")
            f.write("Based on the analysis above, consider the following optimization strategies:\n\n")
            f.write("1. **High-Impact Functions**: Focus optimization efforts on functions with >5% CPU usage\n")
            f.write("2. **Module Analysis**: Investigate modules with disproportionate CPU consumption\n")
            f.write("3. **Algorithm Optimization**: Review algorithms in top CPU-consuming functions\n")
            f.write("4. **Memory Access Patterns**: Optimize data structures and memory layout\n")
            f.write("5. **Compiler Optimizations**: Consider function-specific optimization hints\n\n")
            
            f.write("## Next Steps\n\n")
            f.write("1. Implement targeted optimizations based on hotspot analysis\n")
            f.write("2. Re-profile after optimizations to measure improvement\n")
            f.write("3. Establish performance regression testing\n")
            f.write("4. Document optimization strategies for future development\n")
        
        print(f"   Report generated: {report_file}")
        return report_file
        
    except Exception as e:
        print(f"[ERROR] Failed to generate report: {e}")
        return None


def analyze_etl_file(etl_path, output_dir=None):
    """Analyze a single ETL file and generate reports."""
    if output_dir is None:
        output_dir = Path(etl_path).parent / "analysis"
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Validate inputs
    if not validate_etl_file(etl_path):
        return False
    
    xperf_path = get_xperf_path()
    if not xperf_path:
        print("[ERROR] xperf.exe not found. Please install Windows Performance Toolkit.")
        return False
    
    print(f"Using xperf: {xperf_path}")
    
    # Extract and analyze data
    analysis_file = extract_cpu_sampling_data(etl_path, xperf_path, output_dir)
    hotspots_file = analyze_hotspots(analysis_file, output_dir)
    
    # Generate individual report
    if hotspots_file:
        report_file = generate_report([hotspots_file], output_dir)
        return report_file is not None
    
    return False


def analyze_directory(profile_dir, output_dir=None):
    """Analyze all ETL files in a directory."""
    profile_dir = Path(profile_dir)
    
    if output_dir is None:
        output_dir = profile_dir / "analysis"
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Find all ETL files
    etl_files = list(profile_dir.glob("*.etl"))
    
    if not etl_files:
        print(f"[ERROR] No ETL files found in: {profile_dir}")
        return False
    
    print(f"Found {len(etl_files)} ETL files to analyze")
    
    # Validate xperf
    xperf_path = get_xperf_path()
    if not xperf_path:
        print("[ERROR] xperf.exe not found. Please install Windows Performance Toolkit.")
        return False
    
    print(f"Using xperf: {xperf_path}")
    
    # Process each ETL file
    analysis_files = []
    success_count = 0
    
    for etl_file in etl_files:
        print(f"\nProcessing {etl_file.name}...")
        
        try:
            if validate_etl_file(etl_file):
                analysis_file = extract_cpu_sampling_data(etl_file, xperf_path, output_dir)
                hotspots_file = analyze_hotspots(analysis_file, output_dir)
                
                if hotspots_file:
                    analysis_files.append(hotspots_file)
                    success_count += 1
                    
        except Exception as e:
            print(f"   [ERROR] Failed to process {etl_file.name}: {e}")
    
    # Generate combined report
    if analysis_files:
        print(f"\nGenerating combined analysis report...")
        report_file = generate_report(analysis_files, output_dir)
        
        print(f"\n=== Analysis Complete ===")
        print(f"Processed: {success_count}/{len(etl_files)} files")
        print(f"Output directory: {output_dir}")
        if report_file:
            print(f"Report: {report_file}")
        
        return True
    else:
        print(f"\n[ERROR] No files were successfully analyzed")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="Analyze Dosatsu performance profiles generated by etwprof",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python analyze_profile.py --file dosatsu_profile_simple.etl
  python analyze_profile.py --directory artifacts/profile
  python analyze_profile.py --directory artifacts/profile --output analysis_results
        """
    )
    
    parser.add_argument("--file", metavar="ETL_FILE",
                       help="Analyze a single ETL file")
    parser.add_argument("--directory", metavar="PROFILE_DIR",
                       help="Analyze all ETL files in directory")
    parser.add_argument("--output", metavar="OUTPUT_DIR",
                       help="Output directory for analysis results")
    
    args = parser.parse_args()
    
    if not args.file and not args.directory:
        parser.print_help()
        return 1
    
    success = True
    
    if args.file:
        success &= analyze_etl_file(args.file, args.output)
    
    if args.directory:
        success &= analyze_directory(args.directory, args.output)
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
